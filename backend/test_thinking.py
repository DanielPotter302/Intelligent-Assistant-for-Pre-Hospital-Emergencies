#!/usr/bin/env python3
"""
æµ‹è¯•Qwen3æ€è€ƒåŠŸèƒ½
"""
import asyncio
from app.services.ai_service import AIService

async def test_thinking_feature():
    """æµ‹è¯•æ€è€ƒåŠŸèƒ½"""
    print("å¼€å§‹æµ‹è¯•Qwen3æ€è€ƒåŠŸèƒ½...")
    
    ai_service = AIService()
    
    # æµ‹è¯•æ¶ˆæ¯
    messages = [
        {
            "role": "user",
            "content": "è¯·åˆ†æä¸€ä¸ªå¤æ‚çš„åŒ»ç–—æ€¥æ•‘åœºæ™¯ï¼šä¸€å50å²ç”·æ€§æ‚£è€…çªç„¶å€’åœ°ï¼Œæ„è¯†ä¸æ¸…ï¼Œå‘¼å¸æ€¥ä¿ƒï¼Œé¢è‰²è‹ç™½ï¼Œè¡€å‹90/60mmHgï¼Œå¿ƒç‡120æ¬¡/åˆ†ã€‚è¯·è¯¦ç»†åˆ†æå¯èƒ½çš„ç—…å› å’Œå¤„ç†æ–¹æ¡ˆã€‚"
        }
    ]
    
    print("\n=== æµ‹è¯•Qwen3æ€è€ƒåŠŸèƒ½ ===\n")
    print("å‘é€å¤æ‚é—®ç­”è¯·æ±‚ï¼ˆgraphæ¨¡å¼ï¼‰...")
    print(f"æ¶ˆæ¯: {messages[0]['content'][:50]}...")
    print("=" * 50)
    
    thinking_detected = False
    thinking_content_length = 0
    answer_content_length = 0
    
    try:
        async for chunk in ai_service.stream_chat_completion(messages, mode="graph"):
            if chunk["type"] == "thinking":
                if not thinking_detected:
                    print("ğŸ§  æ£€æµ‹åˆ°æ€è€ƒè¿‡ç¨‹:")
                    thinking_detected = True
                thinking_content_length += len(chunk["content"])
                print(f"æ€è€ƒ: {chunk['content']}", end="", flush=True)
            elif chunk["type"] == "answer_start":
                if thinking_detected:
                    print("\n\nğŸ’¡ å¼€å§‹å›ç­”:")
            elif chunk["type"] == "answer":
                answer_content_length += len(chunk["content"])
                print(chunk["content"], end="", flush=True)
            elif chunk["type"] == "done":
                print("\n\nâœ… å›ç­”å®Œæˆ")
                if thinking_detected:
                    print(f"ğŸ“ æ€è€ƒå†…å®¹é•¿åº¦: {thinking_content_length} å­—ç¬¦")
                break
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
    
    print("\n" + "=" * 50)
    print(f"æ€è€ƒåŠŸèƒ½çŠ¶æ€: {'âœ… å·²å¯ç”¨' if thinking_detected else 'âŒ æœªå¯ç”¨'}")
    print(f"å›ç­”å†…å®¹é•¿åº¦: {answer_content_length} å­—ç¬¦")
    
    # å¯¹æ¯”æµ‹è¯•ï¼šçŸ¥è¯†åº“æ¨¡å¼ï¼ˆä¸åº”è¯¥æœ‰æ€è€ƒè¿‡ç¨‹ï¼‰
    print("\n=== æµ‹è¯•çŸ¥è¯†åº“æ¨¡å¼ï¼ˆå¯¹æ¯”ï¼‰ ===\n")
    print("å‘é€çŸ¥è¯†åº“è¯·æ±‚ï¼ˆkbæ¨¡å¼ï¼‰...")
    print("æ¶ˆæ¯: ä»€ä¹ˆæ˜¯å¿ƒè‚ºå¤è‹ï¼Ÿ")
    print("=" * 30)
    
    kb_thinking_detected = False
    kb_answer_length = 0
    
    try:
        async for chunk in ai_service.stream_chat_completion([{"role": "user", "content": "ä»€ä¹ˆæ˜¯å¿ƒè‚ºå¤è‹ï¼Ÿ"}], mode="kb"):
            if chunk["type"] == "thinking":
                kb_thinking_detected = True
                print(f"ğŸ§  æ€è€ƒ: {chunk['content']}", end="", flush=True)
            elif chunk["type"] == "answer_start":
                print("ğŸ’¡ å¼€å§‹å›ç­”:")
            elif chunk["type"] == "answer":
                kb_answer_length += len(chunk["content"])
                print(chunk["content"], end="", flush=True)
            elif chunk["type"] == "done":
                print("\n\nâœ… å›ç­”å®Œæˆ")
                break
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
    
    print("\n" + "=" * 30)
    print(f"æ€è€ƒåŠŸèƒ½çŠ¶æ€: {'âœ… å·²å¯ç”¨' if kb_thinking_detected else 'âŒ æœªå¯ç”¨'}")
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ” æµ‹è¯•æ€»ç»“:")
    print(f"  å¤æ‚é—®ç­”æ¨¡å¼ (graph): {'âœ… æ€è€ƒåŠŸèƒ½æ­£å¸¸' if thinking_detected else 'âŒ æ€è€ƒåŠŸèƒ½æœªå¯ç”¨'}")
    print(f"  çŸ¥è¯†åº“æ¨¡å¼ (kb): {'âœ… æ­£å¸¸ï¼ˆæ— æ€è€ƒï¼‰' if not kb_thinking_detected else 'âš ï¸ æ„å¤–å¯ç”¨äº†æ€è€ƒåŠŸèƒ½'}")
    
    if thinking_detected and not kb_thinking_detected:
        print("\nğŸ‰ æ€è€ƒåŠŸèƒ½é…ç½®æ­£ç¡®ï¼")
    elif not thinking_detected:
        print("\nâš ï¸ å¤æ‚é—®ç­”æ¨¡å¼çš„æ€è€ƒåŠŸèƒ½æœªå¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®")
    elif kb_thinking_detected:
        print("\nâš ï¸ çŸ¥è¯†åº“æ¨¡å¼æ„å¤–å¯ç”¨äº†æ€è€ƒåŠŸèƒ½ï¼Œè¯·æ£€æŸ¥é…ç½®")

if __name__ == "__main__":
    asyncio.run(test_thinking_feature()) 